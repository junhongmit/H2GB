{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90dd3138-b8c4-4dc8-8676-eaea3beaa9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlcroissant[dev]\n",
      "  Cloning https://github.com/mlcommons/croissant.git (to revision main) to /tmp/pip-install-nvkv2ky_/mlcroissant_bb345f25a00740628eae162fca76aa96\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mlcommons/croissant.git /tmp/pip-install-nvkv2ky_/mlcroissant_bb345f25a00740628eae162fca76aa96\n",
      "  Resolved https://github.com/mlcommons/croissant.git to commit 0f95e04763557929e4f4c6711c108c0d9cf7b818\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: rdflib in /home/junhong/anaconda3/lib/python3.9/site-packages (from mlcroissant[dev]) (7.0.0)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.0 Requires-Python >=3.10; 1.7.0 Requires-Python >=3.10; 1.8.0 Requires-Python >=3.11; 1.9.0 Requires-Python >=3.11; 1.9.1 Requires-Python >=3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement etils[epath]>=1.7.0 (from mlcroissant[dev]) (from versions: 0.1.0, 0.2.0, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.4.0, 0.5.0, 0.5.1, 0.6.0, 0.7.0, 0.7.1, 0.8.0, 0.9.0, 1.0.0, 1.1.0, 1.1.1, 1.2.0, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for etils[epath]>=1.7.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install mlcroissant from the source\n",
    "# !apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\n",
    "!pip install \"git+https://github.com/${GITHUB_REPOSITORY:-mlcommons/croissant}.git@${GITHUB_HEAD_REF:-main}#subdirectory=python/mlcroissant&egg=mlcroissant[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5621c-5ac7-4a85-8a92-dad590c69681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "\n",
    "# FileObjects and FileSets define the resources of the dataset.\n",
    "distribution = [\n",
    "    # gpt-3 is hosted on a GitHub repository:\n",
    "    mlc.FileObject(\n",
    "        id=\"github-repository\",\n",
    "        name=\"github-repository\",\n",
    "        description=\"H2GB repository on GitHub.\",\n",
    "        content_url=\"https://github.com/junhongmit/H2GB\",\n",
    "        encoding_format=\"git+https\",\n",
    "        sha256=\"main\",\n",
    "    ),\n",
    "    # Within that repository, a FileSet lists all JSONL files:\n",
    "    mlc.FileSet(\n",
    "        id=\"jsonl-files\",\n",
    "        name=\"jsonl-files\",\n",
    "        description=\"JSONL files are hosted on the GitHub repository.\",\n",
    "        contained_in=[\"github-repository\"],\n",
    "        encoding_format=\"application/jsonlines\",\n",
    "        includes=\"data/*.jsonl\",\n",
    "    ),\n",
    "]\n",
    "record_sets = [\n",
    "    # RecordSets contains records in the dataset.\n",
    "    mlc.RecordSet(\n",
    "        id=\"jsonl\",\n",
    "        name=\"jsonl\",\n",
    "        # Each record has one or many fields...\n",
    "        fields=[\n",
    "            # Fields can be extracted from the FileObjects/FileSets.\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/context\",\n",
    "                name=\"context\",\n",
    "                description=\"\",\n",
    "                data_types=mlc.DataType.TEXT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    # Extract the field from the column of a FileObject/FileSet:\n",
    "                    extract=mlc.Extract(column=\"context\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/completion\",\n",
    "                name=\"completion\",\n",
    "                description=\"The expected completion of the promt.\",\n",
    "                data_types=mlc.DataType.TEXT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"completion\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/task\",\n",
    "                name=\"task\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.TEXT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(\n",
    "                        file_property=mlc._src.structure_graph.nodes.source.FileProperty.filename\n",
    "                    ),\n",
    "                    # Extract the field from a regex on the filename:\n",
    "                    transforms=[mlc.Transform(regex=\"^(.*)\\.jsonl$\")],\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "# Metadata contains information about the dataset.\n",
    "metadata = mlc.Metadata(\n",
    "    name=\"gpt-3\",\n",
    "    # Descriptions can contain plain text or markdown.\n",
    "    description=(\n",
    "        \"Recent work has demonstrated substantial gains on many NLP tasks and\"\n",
    "        \" benchmarks by pre-training on a large corpus of text followed by\"\n",
    "        \" fine-tuning on a specific task. While typically task-agnostic in\"\n",
    "        \" architecture, this method still requires task-specific fine-tuning\"\n",
    "        \" datasets of thousands or tens of thousands of examples. By contrast,\"\n",
    "        \" humans can generally perform a new language task from only a few\"\n",
    "        \" examples or from simple instructions \\u2013 something which current\"\n",
    "        \" NLP systems still largely struggle to do. Here we show that scaling\"\n",
    "        \" up language models greatly improves task-agnostic, few-shot\"\n",
    "        \" performance, sometimes even reaching competitiveness with prior\"\n",
    "        \" state-of-the-art fine-tuning approaches. Specifically, we train\"\n",
    "        \" GPT-3, an autoregressive language model with 175 billion parameters,\"\n",
    "        \" 10x more than any previous non-sparse language model, and test its\"\n",
    "        \" performance in the few-shot setting. For all tasks, GPT-3 is applied\"\n",
    "        \" without any gradient updates or fine-tuning, with tasks and few-shot\"\n",
    "        \" demonstrations specified purely via text interaction with the model.\"\n",
    "        \" GPT-3 achieves strong performance on many NLP datasets, including\"\n",
    "        \" translation, question-answering, and cloze tasks, as well as several\"\n",
    "        \" tasks that require on-the-fly reasoning or domain adaptation, such as\"\n",
    "        \" unscrambling words, using a novel word in a sentence, or performing\"\n",
    "        \" 3-digit arithmetic. At the same time, we also identify some datasets\"\n",
    "        \" where GPT-3's few-shot learning still struggles, as well as some\"\n",
    "        \" datasets where GPT-3 faces methodological issues related to training\"\n",
    "        \" on large web corpora. Finally, we find that GPT-3 can generate\"\n",
    "        \" samples of news articles which human evaluators have difficulty\"\n",
    "        \" distinguishing from articles written by humans. We discuss broader\"\n",
    "        \" societal impacts of this finding and of GPT-3 in general.\"\n",
    "    ),\n",
    "    cite_as=None,\n",
    "    url=\"https://github.com/openai/gpt-3\",\n",
    "    distribution=distribution,\n",
    "    record_sets=record_sets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1106e1-761f-4e30-8076-8ac20bdc6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata.issues.report())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
